---
title: "Week 7 figures - Lectures 12 and 13"
date: 2025-05-12
categories: [linear models]
citation:
  url: https://spring-2025.envs-193ds.com/lecture/lecture_week-07.html
---

## 0. Set up

```{r libraries}
# cleaning
library(tidyverse)

# visualization
theme_set(theme_classic() +
            theme(panel.grid = element_blank(),
                  axis.text = element_text(size = 18),
                  axis.title = element_text(size = 18),
                  text = element_text(family = "Lato")))
library(patchwork)
library(ggeffects)
library(flextable)
library(modelsummary)
library(gtsummary)
library(readxl)
library(here)
library(janitor)

# analysis
library(car)
library(performance)
library(broom)
```

## 1. Math

### a. Residuals

Residuals are the difference between the actual observed value ($y_i$) and the model prediction ($\hat{y}$) at some value of $x$.

$$
residual = y_i - \hat{y}
$$

Ordinary least squares minimizes the sum of squares of the residuals.

### b. Model equations

OLS gives you an equation for a line.

$$
\begin{align}
y &= b + mx \\

y &= \beta_0 + \beta_1x + \epsilon \\

\end{align}
$$
$\beta$ terms ("betas") are often referred to as "model coefficients". 

### c. Mathematical hypothesis

Statistically, the hypotheses are:  

H_0_: the predictor variable does not predict the response  
H_A_: the predictor variable does predict the response  

Mathematically, you might express that as:

$$
H_0: \beta_1 = 0  \\
H_A: \beta_1 \neq 0
$$

### d. R2

$$
\begin{align}
R^2 &= 1 - \frac{\sum_{i = 1}^{n}(y_i - \hat{y})^2}{\sum_{i = 1}^{n}(y_i - \bar{y})^2} \\
&= 1 - \frac{SS_{residuals}}{SS_{total}} 
\end{align}
$$

### e. Pearson's correlation

## formula for Pearson's correlation

$$
r = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum(x_i-\bar{x})^2}\sqrt{\sum(y_i - \bar{y})^2}}
$$


## test statistic for pearson correlation

$$
\begin{align}
t &= \frac{r\sqrt{n - 2}}{\sqrt{1-r^2}} \\
df &= n -2
\end{align}
$$

## 2. R2

```{r r2-1}
df <- tibble(
  x = seq(from = 1, to = 20, by = 1),
  r2_1 = 3*x + 1,
  r2_between = runif(n = 20, min = 1, max = 5)*x + runif(n = 20, min = 1, max = 5),
  r2_0 = runif(n = 20, min = 1, max = 20)
)

lm(r2_1 ~ x, data = df) |> 
  summary()

ggplot(df,
       aes(x = x,
           y = r2_1)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm")

lm(r2_between ~ x, data = df) |> 
  summary()

ggplot(df,
       aes(x = x,
           y = r2_between)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm",
              se = FALSE)

lm(r2_0 ~ x, data = df) |> 
  summary()

ggplot(df,
       aes(x = x,
           y = r2_0)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm",
              se = FALSE)
```


## 3. Red abalone example

This example is inspired in by Hamilton et al. 2022.  

We work with the linear model using the real data in class, but it's hard to see the difference in diagnostic plots between a "good" vs "bad" linear model with that data set. 

This is the fake data generated to compare different diagnostic plots.

### a. generating the data

```{r abalone-growth-data}
set.seed(666)
abalone <- tibble(
  temperature = seq(from = 10, to = 18, by = 1),
  growth1 = runif(length(temperature), min = -0.7, max = -0.63)*temperature + runif(length(temperature), min = 15, max = 17),
  growth2 = runif(length(temperature), min = -0.7, max = -0.63)*temperature + runif(length(temperature), min = 15, max = 17),
  growth3 = runif(length(temperature), min = -0.7, max = -0.63)*temperature + runif(length(temperature), min = 15, max = 17)
) |> 
  pivot_longer(cols = growth1:growth3,
               names_to = "rep",
               values_to = "growth") |> 
  mutate(growth = round(growth, digits = 1)) |> 
  select(temperature, growth) |> 
  rename(x = temperature,
         y = growth)

# look at your data:
head(abalone, 10)

ggplot(data = abalone,
       aes(x = x,
           y = y)) +
  geom_point() 
```

Seems like there is a linear relationship between temperature and abalone growth. As temperature increases, abalone growth decreases.

### b. fitting a model

```{r abalone-model-and-checks}
abalone_model <- lm(
  y ~ x,
  data = abalone
)

# just checking DHARMa residuals just in case
DHARMa::simulateResiduals(abalone_model, plot = TRUE)

# base R residuals
par(mfrow = c(2, 2))
plot(abalone_model)
```

### c. looking at model coefficients

```{r abalone-summary}
summary(abalone_model)

# common way of representing model summaries
# from flextable package
flextable::as_flextable(abalone_model)

# better table
flextable::as_flextable(abalone_model) |> 
  set_formatter(values = list("p.value" = function(x){ # special function to represent p < 0.001
    z <- scales::label_pvalue()(x)
    z[!is.finite(x)] <- ""
    z
  }))

# somewhat more customizable way
# from modelsummary package
modelsummary(abalone_model)

# better table
modelsummary(list("Abalone model" = abalone_model), # naming the model
             fmt = 2, # rounding digits to 2 decimal places
             estimate = "{estimate} [{conf.low}, {conf.high}] ({p.value})", # customizing appearance
             statistic = NULL, # not displaying standard error
             gof_omit = 'DF|AIC|BIC|Log.Lik.|RMSE') # taking out some extraneous info

# using gtsummary package
tbl_regression(abalone_model,
               intercept = TRUE)

# more customizing
tbl_regression(abalone_model, # model object
               intercept = TRUE) |> # show the intercept
  as_flex_table() # turn it into a flextable (easier to save)
```


```{r abalone-anova}
anova(abalone_model)
```

### d. visualizing the model

```{r abalone-plot}
model_preds <- ggpredict(
  abalone_model,
  terms = "x"
)

# look at the output:
model_preds

ggpredict(abalone_model, 
          terms = "x[18]")

# plotting without 95% CI
ggplot(abalone, # using the actual data
       aes(x = x, # x-axis
           y = y)) + # y-axis
  geom_point(color = "cornflowerblue", # each point is an individual abalone
             size = 3) +
  
  # model prediction: actual model line
  geom_line(data = model_preds, # model prediction table
            aes(x = x, # x-axis
                y = predicted), # y-axis
            linewidth = 1)  # line width


# plotting
ggplot(abalone, # using the actual data
       aes(x = x, # x-axis
           y = y)) + # y-axis
  
  # plot the data first
  # each point is an individual abalone
  geom_point(color = "cornflowerblue",
             size = 3) + 
  
  # model prediction: 95% CI
  geom_ribbon(data = model_preds, # model prediction table
              aes(x = x, # x-axis
                  y = predicted, # y-axis
                  ymin = conf.low, # lower bound of 95% CI
                  ymax = conf.high), # upper bound of 95% CI
              alpha = 0.2) + # transparency) 
  
  # model prediction: actual model line
  geom_line(data = model_preds, # model prediction table
            aes(x = x, # x-axis
                y = predicted), # y-axis
            linewidth = 1) # line width

# compare with:
ggplot(abalone,
       aes(x = x,
           y = y)) +
  geom_point() +
  geom_smooth(method = "lm")
```

### e. outliers

```{r outlier-checks}

abalone |> 
  mutate(outlier = ifelse(row_number() %in% c(19, 21, 27), "yes", "no")) |> 
  ggplot(aes(x = x,
             y = y)) +
  geom_point(aes(color = outlier), 
             size = 3) + 
  geom_line(data = model_preds,
            aes(x = x,
                y = predicted),
            linewidth = 1) +
  scale_color_manual(values = c("yes" = "red", "no" = "cornflowerblue")) +
  theme(legend.position = "none")

# new abalone data frame
abalone2 <- abalone |> 
  slice(-c(19, 21, 27))

abalone_model2 <- lm(y ~ x,
                    data = abalone2)

summary(abalone_model2)

model_preds2 <- ggpredict(abalone_model2, terms = "x")

ggplot(data = abalone2,
       aes(x = x,
           y = y)) +
  geom_point(color = "cornflowerblue",
             size = 3) +
  geom_line(data = model_preds2,
            aes(x = x,
                y = predicted), 
            linewidth = 1)
```

### f. actual data

Data from: Hamilton et al. 2022. _Aquaculture_. "Integrated multi-trophic aquaculture mitigates the effects of ocean acidification: Seaweeds raise system pH and improve growth of juvenile abalone."  

Thank you to Scott for being willing to share this data!

```{r hamilton-et-al-data}
#| warning: false

abalone <- read_xlsx(here("data", "Abalone IMTA_growth and pH.xlsx")) |> 
  clean_names()

# creating clean dataset
abalone_clean <- abalone |> # start with abalone object
  # select columns of interest
  select(mean_p_h, change_in_area_mm2_d_1_25, recirculation_treatment) |> 
  # rename columns
  rename(mean_ph = mean_p_h, 
         change_in_area = change_in_area_mm2_d_1_25,
         recirculation = recirculation_treatment) |> 
  # recod recirculation
  mutate(recirculation = case_when(
    recirculation == 0 ~ "0% recirculation",
    recirculation == 0.3 ~ "30% recirculation",
    recirculation == 0.65 ~ "65% recirculation"
  ))

# base layer: ggplot
ggplot(data = abalone_clean,
       aes(x = mean_ph,
           y = change_in_area)) +
  # first layer: points representing abalones
  geom_point(size = 4,
             stroke = 1,
             fill = "firebrick3",
             shape = 21)

abalone_model <- lm(
  change_in_area ~ mean_ph, # formula: change in area as a function of pH
  data = abalone_clean      # data frame: abalone_clean
)

DHARMa::simulateResiduals(abalone_model) |> 
  plot()

par(mfrow = c(2, 2))   # creating a 2x2 grid
plot(abalone_model)    # plot diagnostic plots

# more information about the model
summary(abalone_model)

# creating a new object called abalone_preds
abalone_preds <- ggpredict(
  abalone_model,      # model object
  terms = "mean_ph"   # predictor (in quotation marks)
)

# display the predictions
abalone_preds

# look at the column names
colnames(abalone_preds)

# look at the "class" (i.e. "type") of object
class(abalone_preds)

# base layer: ggplot
# using clean data frame
ggplot(data = abalone_clean,
       aes(x = mean_ph,
           y = change_in_area)) +
  # first layer: points representing abalones
  geom_point(size = 4,
             stroke = 1,
             fill = "firebrick3",
             shape = 21) +
  # second layer: ribbon representing confidence interval
  # using predictions data frame
  geom_ribbon(data = abalone_preds,
              aes(x = x,
                  y = predicted,
                  ymin = conf.low,
                  ymax = conf.high),
              alpha = 0.1) +
  # third layer: line representing model predictions
  # using predictions data frame
  geom_line(data = abalone_preds,
            aes(x = x,
                y = predicted),
            linewidth = 1) +
  # axis labels
  labs(x = "Mean pH", 
       y = expression("Change in shell area ("*mm^{2}~d^-1*")"))
```

## 4. Exponential growth example

To compare with abalone example

```{r exp-data-and-model}
df_ex <- tibble(
  x = seq(from = 10, to = 18, length = 27),
  y = c(15, 15, 15, 
        15, 14.3, 14.2, 
        14.1, 14, 13.9,
        13.9, 13.8, 13.7,
        13.2, 13.1, 13,
        12.5, 11.1, 10,
        9.9, 7, 5,
        3, 1.7, 1,
        0.5, 0.3, 0.1)
) 

lm_ex <- lm(y ~ x, data = df_ex)

summary(lm_ex)

ggplot(data = df_ex,
       aes(x = x,
           y = y)) +
  geom_point()

lm_pred <- ggpredict(lm_ex, terms = ~x)

ex_plot_noline <- ggplot(df_ex, aes(x= x, y = y)) +
  geom_point(size = 3, color = "orange") +
  theme_classic() +
  theme(text = element_text(size = 14))

ex_plot_noline

ex_plot <- ggplot(df_ex, aes(x= x, y = y)) +
  geom_point(size = 3, color = "orange") +
  geom_line(data = lm_pred, aes(x = x, y = predicted), linewidth = 1) +
  theme(text = element_text(size = 14)) 

ex_plot
```

```{r exponential-diagnostics}
par(mfrow = c(2, 2))
plot(lm_ex)


DHARMa::simulateResiduals(lm_ex) |> plot()
```

```{r}
#| eval: false

# if using quarto, don't label chunk with a table... so weird
anova_tbl <- broom::tidy(anova(model1)) |> 
  mutate(across(where(is.numeric), ~ round(.x, digits = 2))) |> 
  mutate(p.value = case_when(
    p.value < 0.001 ~ "< 0.001"
  )) 

flextable(anova_tbl) |> 
  set_header_labels(term = "Term", 
                    df = "Degrees of freedom", 
                    sumsq = "Sum of squares", 
                    meansq = "Mean squares", 
                    statistic = "F-statistic", 
                    p.value = "p-value") |> 
  set_table_properties(layout = "autofit", width = 0.8)
```


# 5. Temperature/elevation example

```{r sonadora-temperature}

# data
sonadora <- read_csv(here::here("data", "knb-lter-luq.183.877108", "Temp_SonadoraGradient_Daily.csv"))

# cleaning
sonadora_clean <- sonadora |> 
  janitor::clean_names() |> 
  pivot_longer(cols = plot_250:plot_1000,
               names_to = "plot_name",
               values_to = "temp_c") |> 
  separate_wider_delim(cols = plot_name,
                       delim = "_",
                       names = c("plot", "elevation_m"),
                       cols_remove = FALSE) |> 
  select(-plot) |> 
  mutate(elevation_m = as.numeric(elevation_m))

# summarizing
sonadora_sum <- sonadora_clean |> 
  group_by(plot_name, elevation_m) |> 
  reframe(mean_temp_c = mean(temp_c, na.rm = TRUE)) |> 
  arrange(elevation_m)

# model
model <- lm(mean_temp_c ~ elevation_m, data = sonadora_sum)

# visualization
ggplot(data = sonadora_sum, 
       aes(x = elevation_m,
           y = mean_temp_c)) +
  geom_point(size = 3) +
  labs(x = "Elevation (m)",
       y = "Temperature (°C)")

# diagnostics from DHARMa
DHARMa::simulateResiduals(model, plot = TRUE)

# summary
summary(model)
tidy(model)

# model predictions
ggpredict(model) |> plot(show_data = TRUE)
```

```{r sonadora-diagnostics}
#| fig-width: 12
#| fig-height: 12

# diagnostics
par(mfrow = c(2, 2))
plot(model)
```

# 6. correlation

### a. abalone correlation

```{r abalone-correlation}
cor.test(abalone_clean$mean_ph, abalone_clean$change_in_area,
         method = "pearson")
```

### b. correlation but different slopes

```{r correlation}
#| fig-width: 12
#| fig-height: 8

corr_df <- tibble(
  x = seq(from = 1, to = 10, by = 0.5),
  y1 = 0.25*x,
  y2 = 1*x
) 

corr_0.25 <- ggplot(data = corr_df,
                    aes(x = x,
                        y = y1)) +
  geom_point(size = 3) +
  scale_y_continuous(limits = c(0, 10))

corr_1 <- ggplot(data = corr_df,
                 aes(x = x,
                     y = y2)) +
  geom_point(size = 3) +
  scale_y_continuous(limits = c(0, 10))

corr_0.25 + corr_1

```


### c. no correlation but clear relationship

```{r parabola}
x_lm <- seq(from = 1, to = 30, length.out = 50)
# y = a( x – h) 2 + k
df_para <- cbind(
  x = x_lm,
  y = 0.1*(x_lm - 15)^2 + 12
) |> 
  as_tibble()

ggplot(data = df_para, 
       aes(x = x, 
           y = y)) +
  geom_point(size = 3) 

cor.test(df_para$x, df_para$y, method = "pearson")

```


# exponential growth example

```{r old-exp-data-and-model}
x_ex <- seq(from = 5, to = 9, length = 30)

y_ex <- exp(x_ex)

df_ex <- cbind(
  x = x_ex,
  y = -exp(x_ex)
) |> 
  as_tibble()

lm_ex <- lm(y ~ x, data = df_ex)

lm_ex
```

## model summary
```{r exp-model-summary}
summary(lm_ex)
```

## model plots

```{r exp-model-plots}
lm_pred <- ggpredict(lm_ex, terms = ~x)

ex_plot_noline <- ggplot(df_ex, aes(x= x, y = y)) +
  geom_point(shape = 17, size = 3, color = "orange") +
  theme_classic() +
  theme(text = element_text(size = 14))

ex_plot <- ggplot(df_ex, aes(x= x, y = y)) +
  geom_point(shape = 17, size = 3, color = "orange") +
  geom_line(data = lm_pred, aes(x = x, y = predicted), linewidth = 1) +
  theme_classic() +
  theme(text = element_text(size = 14))
```

# diagnostic plots
```{r diagnostics}
#| eval: false
#| 
par(mfrow = c(2, 4))
plot(model1, which = c(1), col = "cornflowerblue", pch = 19)
plot(lm_ex, which = c(1), col = "orange", pch = 17)
plot(model1, which = c(2), col = "cornflowerblue", pch = 19)
plot(lm_ex, which = c(2), col = "orange", pch = 17)
plot(model1, which = c(3), col = "cornflowerblue", pch = 19)
plot(lm_ex, which = c(3), col = "orange", pch = 17)
plot(model1, which = c(5), col = "cornflowerblue", pch = 19)
plot(lm_ex, which = c(5), col = "orange", pch = 17)
dev.off()
```




