---
title: "Coding workshop: Week 8 and 9"
description: "multiple linear regression, model selection"
freeze: auto
format:
  html:
    toc: true
    toc-location: left
    code-fold: true
    theme: yeti
execute:
  message: false
  warning: false
author:
  - name: An Bui
    url: https://an-bui.com/
    affiliation: UC Santa Barbara, Ecology, Evolution, and Marine Biology
    affiliation-url: https://www.eemb.ucsb.edu/
date: 2023-05-24
categories: [multiple linear regression, pitcher plants]
citation:
  url: https://an-bui.github.io/ES-193DS-W23/workshop/workshop-08_2023-05-24.html
---

# Notes from workshop

## YAML options for a quarto document

```
---
title: "Untitled"
format:
  html:
    toc: true
    toc-location: left
    code-fold: true
    theme: yeti
execute:
  message: false
  warning: false
---
```

## YAML options for an RMarkdown document:

```
---
title: "Untitled"
format:
  html_document:
    toc: true
    toc-location: left
    code_folding: true
    theme: yeti
---
```

## knitr set up options for an RMarkdown document:

```{r knitr-example, eval = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


# Set up

```{r libraries}
# should haves (from last week)
library(tidyverse)
library(here)
library(janitor)
library(ggeffects)
library(performance)
library(naniar) # or equivalent
library(flextable) # or equivalent
library(car)
library(broom)
# would be nice to have
library(corrplot)
library(AICcmodavg)
library(GGally)
```

# Read in the data:

```{r reading-data}
plant <- read_csv(here("data", "knb-lter-hfr.109.18", "hf109-01-sarracenia.csv")) %>% 
  # make the column names cleaner
  clean_names() %>% 
  # selecting the columns of interest
  select(totmass, species, feedlevel, sla, chlorophyll, amass, num_lvs, num_phylls)
```

# Visualize the missing data:

```{r missing-data-visualization}
gg_miss_var(plant)
```

# Subsetting the data by dropping NAs:

```{r subset-drop-NA}
plant_subset <- plant %>% 
  drop_na(sla, chlorophyll, amass, num_lvs, num_phylls)
```

# Create a correlation plot:  

(example writing) To determine the relationships between numerical variables in our dataset, we calculated Pearsons r and visually represented correlation using a correlation plot.

```{r correlation-plot}
# calculate Pearson's r for numerical values only
plant_cor <- plant_subset %>% 
  select(feedlevel:num_phylls) %>% 
  cor(method = "pearson")
  
# creating a correlation plot
corrplot(plant_cor,
         # change the shape of what's in the cells
         method = "ellipse",
         addCoef.col = "black"
         )
```

# Create a plot of each varable compared against the others 

```{r pairs-plot}
plant_subset %>% 
  select(species:num_phylls) %>% 
  ggpairs()
```

# Starting regression here:  

(example) To determine how species and physiological characteristics predict biomass, we fit multiple linear models.

```{r null-and-full-models}
null <- lm(totmass ~ 1, data = plant_subset)
full <- lm(totmass ~ species + feedlevel + sla + chlorophyll + amass + num_lvs + num_phylls, data = plant_subset)
```

# Diagnostics

We visually assess normality and homoskedasticity of residuals using diagnostic plots for the full model:

```{r full-diagnostics}
par(mfrow = c(2, 2))
plot(full)
```

We also tested for normality using the Shapiro-Wilk test (null hypothesis: variable of interest (i.e. the residuals) are normally distributed).  

We tested for heteroskedasticity using the Breusch-Pagan test (null hypothesis: variable of interest has constant variance).
```{r}
check_normality(full)
check_heteroscedasticity(full)
```

```{r model-logs}
null_log <- lm(log(totmass) ~ 1, data = plant_subset)
full_log <- lm(log(totmass) ~ species + feedlevel + sla + chlorophyll + amass + num_lvs + num_phylls, data = plant_subset)

plot(full_log)
check_normality(full_log)
check_heteroscedasticity(full_log)
```

Evaluate multicollinearity:

```{r calculate-vif}
car::vif(full_log)
```

We evaluated multicollinearity by calculating generalized variance inflation factor and determined that...  

try some more models:  

addressing the question: what set of predictor variables best explains the response?

```{r}
model2_log <- lm(log(totmass) ~ species, data = plant_subset)
```

check assumptions for model 2:
```{r}
plot(model2_log)

check_normality(model2_log)
check_heteroscedasticity(model2_log)
```

compare models using Akaike's Information criterion (AIC) values: 

```{r}
AICc(full_log)
AICc(model2_log)
AICc(null_log)

MuMIn::AICc(full_log, model2_log, null_log)
MuMIn::model.sel(full_log, model2_log, null_log)
```

we compared models using AIC and chose the model with the lowest value, which was...

# Results

We found that the ______ model including ___ ____ __ predictors best predicted _______ (model summary).

```{r}
summary(full_log)

table <- tidy(full_log, conf.int = TRUE) %>% 
  # change the p-value numbers if they're really small
  # change the estmaes, standard error, and t-tstatistics to round to ___ digits
  # using mutate
  # make it into a flextable
  flextable() %>% 
  # fit it to the viewer
  autofit()

table
```

use `ggpredict()` to backtranform estimates
```{r}
model_pred <- ggpredict(full_log, terms = "species", back.transform = TRUE)

plot(ggpredict(full_log, terms = "species", back.transform = TRUE), add.data = TRUE)

plot(ggpredict(full_log, terms = "chlorophyll", back.transform = TRUE), add.data = TRUE)

plot(ggpredict(full_log, terms = "sla", back.transform = TRUE), add.data = TRUE)

model_pred
```

